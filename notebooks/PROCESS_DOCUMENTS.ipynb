{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f455435-4a14-481a-abe6-f8744ccaa661",
   "metadata": {
    "collapsed": false,
    "name": "Processing_GP_Docments"
   },
   "source": [
    "# PROCESSING DOCUMENTS\n",
    "\n",
    "You should now have built three models using the DOCUMENT AI Model creation tool: \n",
    "\n",
    "- **GP_NOTES** \n",
    "\n",
    "- **COVID_VACCINATION_CONSENT_FORM**\n",
    "\n",
    "- **RESEARCH_PAPERS**\n",
    "\n",
    "Now, the next stage is to process multiple documents using these models.\n",
    "\n",
    "## MODEL 1: PROCESS GP NOTES\n",
    "\n",
    "Run the query below to view all the scanned in GP notes which are currently residing in a Snowflake Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e54aa7-b9c3-491f-ab0f-312e99711a6b",
   "metadata": {
    "language": "sql",
    "name": "list_images"
   },
   "outputs": [],
   "source": [
    "select BUILD_SCOPED_FILE_URL('@DOCUMENT_AI.GP_NOTES',RELATIVE_PATH), * from directory(@DOCUMENT_AI.GP_NOTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047c5fa-9518-4518-a70d-1bb9991f8fb7",
   "metadata": {
    "collapsed": false,
    "name": "proces_docs"
   },
   "source": [
    "### Process Documents from Document AI\n",
    "You will now use the model previously created to process these documents.  Each document will produce meta data under the column name **DOC_META** - this will consist of all the fields that was built in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c59f0-df36-4cf6-b01c-38424279b1b5",
   "metadata": {
    "language": "sql",
    "name": "process_gp_notes"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS DEFAULT_SCHEMA.GP_NOTES_RAW AS\n",
    "\n",
    "SELECT *, DEFAULT_DATABASE.DOCUMENT_AI.GP_NOTES!PREDICT(\n",
    "  GET_PRESIGNED_URL(@DEFAULT_DATABASE.DOCUMENT_AI.GP_NOTES, RELATIVE_PATH), 1) DOC_META\n",
    "FROM DIRECTORY(@DEFAULT_DATABASE.DOCUMENT_AI.GP_NOTES);\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.GP_NOTES_RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a613af0-54a1-4e4c-b691-3b99269d33f1",
   "metadata": {
    "collapsed": false,
    "name": "Format_the_view"
   },
   "source": [
    "Let's now make this more readable as a structured table.  you will note that the GP notes have been transformed into a list.  Any field can have a list.  The SQL below only views the first value for all fields except the **GP_NOTES** column do to it containing multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173dddb-8688-40b8-bbea-aa7488411102",
   "metadata": {
    "language": "sql",
    "name": "view_meta_data"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DEFAULT_SCHEMA.REPORTS_STRUCTURED AS \n",
    "select RELATIVE_PATH,\n",
    "\n",
    "DOC_META:__documentMetadata:ocrScore OCR_SCORE,\n",
    "DOC_META:NHS_NUMBER[0]:value::text NHS_NUMBER,\n",
    "DOC_META:APPOINTMENT_DATE[0]:value::text APPOINTMENT_DATE,\n",
    "DOC_META:CONSULTANT[0]:value::text CONSULTANT,\n",
    "DOC_META:NOTES GP_NOTES,\n",
    "DOC_META:PATIENT_NAME[0]:value::text PATIENT_NAME,\n",
    "DOC_META:PRACTICE_ADDRESS[0]:value::text PRACTICE_ADDRESS,\n",
    "DOC_META:PRACTICE_PHONE[0]:value::text PRACTICE_PHONE,\n",
    "DOC_META:PRESCRIPTION[0]:value::text PRESCRIPTION,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from DEFAULT_SCHEMA.GP_NOTES_RAW;\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.REPORTS_STRUCTURED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df60eb2-d067-4942-be32-c883eb566bf5",
   "metadata": {
    "collapsed": false,
    "name": "data_cleaning"
   },
   "source": [
    "## DATA CLEANING\n",
    "\n",
    "We will now only extract all the notes and put it in the same field.  Each note will have a seperate line.  The function **REDUCE** allows you to pick out certain aspects of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226ac36-cb57-47cd-b9ed-cde00c0591cb",
   "metadata": {
    "language": "sql",
    "name": "REDUCE_to_get_all_notes_from_list"
   },
   "outputs": [],
   "source": [
    "SELECT * EXCLUDE GP_NOTES,\n",
    "       REDUCE(o.GP_NOTES,\n",
    "              '',\n",
    "              (acc, val) -> val:value || '\\n' || acc) GP_NOTES\n",
    "  FROM DEFAULT_SCHEMA.REPORTS_STRUCTURED o;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba868fd3-9bfb-4c79-be28-bdae37bdcdf8",
   "metadata": {
    "collapsed": false,
    "name": "clean_date_narritive"
   },
   "source": [
    "One of the other columns that is not consistant is the appointment date column, with the notes originating from a hand written form, formatting may not be logical.  Perhaps a trained model could help decifer this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ef465-9365-4c85-9308-5f6914bae6ec",
   "metadata": {
    "language": "sql",
    "name": "cleaaned_table"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS DEFAULT_SCHEMA.GP_NOTES_CLEANED AS\n",
    "SELECT REPLACE(NHS_NUMBER,' ','') NHS_NUMBER,\n",
    "\n",
    "SNOWFLAKE.CORTEX.COMPLETE('claude-3-5-sonnet',\n",
    "\n",
    "CONCAT('convert the following date into a standardised date format such as YYYY-MM--DD',\n",
    "            APPOINTMENT_DATE,'ONLY RETURN THE DATE WITH NO COMMENTARY'\n",
    "            )) APPOINTMENT_DATE, \n",
    "\n",
    "\n",
    "* EXCLUDE (GP_NOTES,APPOINTMENT_DATE, OCR_SCORE,NHS_NUMBER),\n",
    "       REDUCE(o.GP_NOTES,\n",
    "              '',\n",
    "              (acc, val) -> val:value || '\\n' || acc) GP_NOTES\n",
    "  FROM DEFAULT_SCHEMA.REPORTS_STRUCTURED o;\n",
    "\n",
    "  SELECT * FROM DEFAULT_SCHEMA.GP_NOTES_CLEANED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b50de-8e0d-4543-b570-1637d7328ad0",
   "metadata": {
    "collapsed": false,
    "name": "heading_consent"
   },
   "source": [
    "## MODEL 2: PROCESS COVID TRIAL CONSENT FORMS\n",
    "\n",
    "Let's now run the model to process the trial consent forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9250e-cad9-4841-9609-2880fc0135c8",
   "metadata": {
    "language": "sql",
    "name": "process_consent_forms"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS DEFAULT_SCHEMA.COVID_TRIAL_CONSENT_RAW AS \n",
    "SELECT \n",
    "SIZE, \n",
    "LAST_MODIFIED,\n",
    "RELATIVE_PATH, \n",
    "DEFAULT_DATABASE.DOCUMENT_AI.COVID_VACCINATION_CONSENT_FORM!PREDICT(\n",
    "  GET_PRESIGNED_URL(@DEFAULT_DATABASE.DOCUMENT_AI.CONSENT_FORMS, \n",
    "  RELATIVE_PATH), 1) DOC_META\n",
    "FROM DIRECTORY(@DEFAULT_DATABASE.DOCUMENT_AI.CONSENT_FORMS);\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.COVID_TRIAL_CONSENT_RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a38101-2260-43a7-8a23-ce0b62f1d48c",
   "metadata": {
    "collapsed": false,
    "name": "Flatten_out_Data"
   },
   "source": [
    "### Flattening out the data and creating a structured table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bee4b-84f3-4a1a-af04-7d67e63cacee",
   "metadata": {
    "language": "sql",
    "name": "view_creation"
   },
   "outputs": [],
   "source": [
    "--- Create a table for the processed information\n",
    "CREATE OR REPLACE VIEW DEFAULT_DATABASE.DEFAULT_SCHEMA.COVID_VACCINE_CONSENTS AS \n",
    "SELECT DOC_META,SIZE,LAST_MODIFIED,\n",
    "DOC_META:CHILDBEARING_F[0]:value::text \"Female Patient Childbearing Age (Y/N)\"\n",
    ",DOC_META:CONSENT[0]:value::text \"CONSENT (Y/N)\" \n",
    ",DOC_META:DATE_BIRTH[0]:value::text \" Date of Birth\"\n",
    ",DOC_META:GENDER[0]:value::text \"Gender\"\n",
    ",DOC_META:GP_NAME[0]:value::text \"Name of Surgery\"\n",
    ",DOC_META:INJECTION[0]:value::text \"Injection Arm\"\n",
    ",DOC_META:NHS_NUMBER[0]:value::text \"NHS Number\"\n",
    ",DOC_META:PATIENT_NAME[0]:value::text \"Patient Name\"\n",
    ",DOC_META:REASON[0]:value::text \"Reason for no vaccine\"\n",
    ",DOC_META:Brand_vaccine[0]:value::text \"Vaccination Brand\"\n",
    ",relative_path\n",
    "FROM DEFAULT_SCHEMA.COVID_TRIAL_CONSENT_RAW;\n",
    "\n",
    "-- checking the table\n",
    "SELECT * FROM DEFAULT_SCHEMA.COVID_VACCINE_CONSENTS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b8c10-6991-4ca1-8b30-fc4d34519f4d",
   "metadata": {
    "collapsed": false,
    "name": "title_visualise_quality"
   },
   "source": [
    "### VISUALISING DOCUMENT PROCESSING QUALITY SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2dcfcd-f61d-433a-9def-9484e91321b0",
   "metadata": {
    "language": "sql",
    "name": "quality_view"
   },
   "outputs": [],
   "source": [
    "-- Create a table with all values and scores\n",
    "CREATE OR REPLACE VIEW DEFAULT_SCHEMA.covid_ocr_score2\n",
    "AS\n",
    "WITH \n",
    "-- First part gets the result from applying the model on the pdf documents as a JSON with additional metadata\n",
    "temp as(\n",
    "    SELECT * FROM DEFAULT_SCHEMA.COVID_VACCINE_CONSENTS\n",
    "    \n",
    ")\n",
    "-- Second part extract the values and the scores from the JSON into columns\n",
    "SELECT\n",
    "\n",
    "RELATIVE_PATH AS file_name\n",
    ", SIZE AS file_size\n",
    ", last_modified\n",
    ", GET_PRESIGNED_URL(@DEFAULT_DATABASE.DOCUMENT_AI.CONSENT_FORMS, RELATIVE_PATH) snowflake_file_url\n",
    ", DOC_META AS JSON\n",
    ", json:__documentMetadata.ocrScore::FLOAT AS ocrScore\n",
    ", json:CHILDBEARING_F[0]:value::STRING as Female_Patient_Childbearing\n",
    ", json:CHILDBEARING_F[0]:score::FLOAT AS CHILDBEARING_F_score\n",
    ", json:CONSENT[0]:value::STRING as CONSENT\n",
    ", json:CONSENT[0]:score::FLOAT AS CONSENT_score\n",
    ", json:DATE_BIRTH[0]:value::STRING as DATE_BIRTH\n",
    ", json:DATE_BIRTH[0]:score::FLOAT AS DATE_BIRTH_score\n",
    ", json:GENDER[0]:value::STRING as GENDER\n",
    ", json:GENDER[0]:score::FLOAT AS GENDER_score\n",
    ", json:GP_NAME[0]:value::STRING as GP_NAME\n",
    ", json:GP_NAME[0]:score::FLOAT AS GP_NAME_score\n",
    ", json:INJECTION[0]:value::STRING as INJECTION\n",
    ", json:INJECTION[0]:score::FLOAT AS INJECTION_score\n",
    ", json:NHS_NUMBER[0]:value::STRING as NHS_NUMBER\n",
    ", json:NHS_NUMBER[0]:score::FLOAT AS NHS_NUMBER_score\n",
    ", json:PATIENT_NAME[0]:value::STRING as PATIENT_NAME\n",
    ", json:PATIENT_NAME[0]:score::FLOAT AS PATIENT_NAME_score\n",
    ", json:REASON[0]:value::STRING as REASON\n",
    ", json:REASON[0]:score::FLOAT AS REASON_score\n",
    "FROM temp;\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.covid_ocr_score2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1197c0-3f61-4cd9-ac6d-61a28653405c",
   "metadata": {
    "collapsed": false,
    "name": "view_streamlit"
   },
   "source": [
    "Finally we view the final view in a streamlit visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070b225-2d8d-4e82-822c-895cd39a1881",
   "metadata": {
    "language": "python",
    "name": "STREAMLIT"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "import streamlit as st\n",
    "session = get_active_session()\n",
    "table = session.table('DEFAULT_SCHEMA.covid_ocr_score2')\n",
    "\n",
    "st.markdown('### DATA QUALITY SCORES')\n",
    "col1,col2,col3,col4 = st.columns(4)\n",
    "with col1:\n",
    "    st.markdown('INJECTION')\n",
    "    st.bar_chart(table, y='INJECTION_SCORE',x='FILE_NAME', color='#fc8702',x_label='',y_label='')\n",
    "with col2:\n",
    "    st.markdown('GP NAME')\n",
    "    st.bar_chart(table, y='GP_NAME_SCORE',x='FILE_NAME', color='#7bcbff',x_label='',y_label='')\n",
    "with col3:\n",
    "    st.markdown('GENDER')\n",
    "    st.bar_chart(table, y='GENDER_SCORE',x='FILE_NAME', color ='#ba78e5',x_label='',y_label='')\n",
    "\n",
    "with col4:\n",
    "    st.markdown('REASON')\n",
    "    st.bar_chart(table, y='REASON_SCORE',x='FILE_NAME', color='#fc8702',x_label='',y_label='')\n",
    "\n",
    "col1,col2,col3,col4 = st.columns(4)\n",
    "with col1:\n",
    "    st.markdown('NHS NUMBER')\n",
    "    st.bar_chart(table, y='NHS_NUMBER_SCORE',x='FILE_NAME', color='#7bcbff',x_label='',y_label='')\n",
    "with col2:\n",
    "    st.markdown('PATIENT NAME')\n",
    "    st.bar_chart(table, y='PATIENT_NAME_SCORE',x='FILE_NAME', color ='#ba78e5',x_label='',y_label='')\n",
    "\n",
    "with col3:\n",
    "    st.markdown('DATE OF BIRTH')\n",
    "    st.bar_chart(table, y='DATE_BIRTH_SCORE',x='FILE_NAME', color='#7bcbff',x_label='',y_label='')\n",
    "with col4:\n",
    "    st.markdown('CONSENT')\n",
    "    st.bar_chart(table, y='CONSENT_SCORE',x='FILE_NAME', color ='#ba78e5',x_label='',y_label='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397d48c-d850-44de-a21e-4289c65c250b",
   "metadata": {
    "collapsed": false,
    "name": "model_3_title"
   },
   "source": [
    "## MODEL 3: PROCESS RESEARCH PAPERS\n",
    "\n",
    "This model has basic information about each document.  Once we have retrieved the results of this, we will utilise **CORTEX_PARSE_DOCUMENT** to extract all the text from each document.\n",
    "\n",
    "## Extract Key Data from document AI\n",
    "So Same as before, we are extracting the document AI specific fields.  Note that two of the fields contain lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba1f9c-0908-45dc-b60e-5aec323a075c",
   "metadata": {
    "language": "sql",
    "name": "process_research_papers"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS DEFAULT_SCHEMA.RESEARCH_PAPERS_RAW AS \n",
    "SELECT *, DEFAULT_DATABASE.DOCUMENT_AI.RESEARCH_PAPERS!PREDICT(\n",
    "  GET_PRESIGNED_URL(@DEFAULT_DATABASE.DOCUMENT_AI.RESEARCH_PAPERS, RELATIVE_PATH), 2) DOC_META\n",
    "FROM DIRECTORY(@DEFAULT_DATABASE.DOCUMENT_AI.RESEARCH_PAPERS);\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.RESEARCH_PAPERS_RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b094b8-16eb-45c9-a962-6b5db74e9ed3",
   "metadata": {
    "collapsed": false,
    "name": "formatting_table_model_3"
   },
   "source": [
    "Now, the table is formatted go extract the fields into columns.  Where the field contains a list of answers, the **REDUCE** function is used to only extract the field values.  These lists are useful for the search service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc0b54-e6db-4927-8130-2bd4c49dff24",
   "metadata": {
    "language": "sql",
    "name": "formatting_table"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DEFAULT_SCHEMA.RESEARCH_PAPERS_PARSED AS \n",
    "select RELATIVE_PATH,\n",
    "\n",
    "DOC_META:__documentMetadata:ocrScore OCR_SCORE,\n",
    "\n",
    "STRTOK_TO_ARRAY(\n",
    "REDUCE(DOC_META:AUTHOR,\n",
    "              '',\n",
    "              (acc, val) -> val:value || ',' || acc),',') AUTHORS,\n",
    "\n",
    "DOC_META:TITLE_OF_PAPER[0]:value::text TITLE_OF_PAPER,\n",
    "DOC_META:DATE_PUBLICATION[0]:value::text DATE_PUBLICATION,\n",
    "DOC_META:PUBLICATION_LOCATION[0]:value::text PUBLICATION_LOCATION,\n",
    "\n",
    "STRTOK_TO_ARRAY(\n",
    "REDUCE(DOC_META:MORBIDITIES,\n",
    "              '',\n",
    "              (acc, val) -> val:value || ',' || acc),',') MORBIDITIES\n",
    "\n",
    "\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from DEFAULT_SCHEMA.RESEARCH_PAPERS_RAW;\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.RESEARCH_PAPERS_PARSED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e591be-fe59-4119-912c-971aa34d859a",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "Let's now create a new table which adds one additional column.  This column will contain ALL the text from all the documents.  The function **CORTEX.PARSE.DOCUMENT** is used for this.  You have two mode options - **LAYOUT** extracts the layout of the document into markdown and **ocr** only extracts the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b0fdf-2171-4f8c-9ad3-83d8aceec57a",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA AS \n",
    "\n",
    "select * exclude (layout, OCR_SCORE) from (\n",
    "\n",
    "SELECT *, \n",
    "\n",
    "    build_stage_file_url(@DEFAULT_DATABASE.DOCUMENT_AI.RESEARCH_PAPERS,RELATIVE_PATH) URL,\n",
    "    \n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n",
    "                                '@DEFAULT_DATABASE.DOCUMENT_AI.RESEARCH_PAPERS',\n",
    "                                RELATIVE_PATH,\n",
    "                                {'mode': 'LAYOUT'} )  AS LAYOUT, LAYOUT:content::text CONTENT, LAYOUT:metadata:pageCount PAGE_COUNT\n",
    "        \n",
    "FROM DEFAULT_SCHEMA.RESEARCH_PAPERS_PARSED);\n",
    "\n",
    "\n",
    "CREATE OR REPLACE VIEW DOCUMENT_AI.RESEARCH_PAPERS_ALL_DATA AS \n",
    "SELECT * FROM DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA;\n",
    "\n",
    "SELECT * FROM DOCUMENT_AI.RESEARCH_PAPERS_ALL_DATA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf549a-2109-46d6-9c08-f3a7e0237e2e",
   "metadata": {
    "collapsed": false,
    "name": "view_parsed_docs"
   },
   "source": [
    "Now you will view the parsed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04b8b1-ac85-4ec1-a9d3-899d0a566de0",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "view_what_this_looks_like"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "\n",
    "\n",
    "st.title(\"Research Papers\")\n",
    "session = get_active_session()\n",
    "\n",
    "side_letters = session.table('DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA').select('RELATIVE_PATH')#.filter(F.col('RELATIVE_PATH').like('ANALYST_REPORTS%'))\n",
    "file_id = st.selectbox('Select Report:', side_letters)\n",
    "doc_details = session.table('DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA').filter(F.col('RELATIVE_PATH')==file_id).limit(1)\n",
    "doc_detailsspd = doc_details.to_pandas()\n",
    "\n",
    "\n",
    "st.markdown('#### Report Details')\n",
    "col1,col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(f'''__Report Date:__ {doc_detailsspd.DATE_PUBLICATION.iloc[0]}''')\n",
    "    st.markdown(f'''__Title of Paper:__ {doc_detailsspd.TITLE_OF_PAPER.iloc[0]}''')\n",
    "    st.markdown(f'''__Author(s):__ {doc_details.select(F.array_to_string(F.col('AUTHORS'),F.lit(','))).collect()[0][0]}''')\n",
    "    \n",
    "with col2:\n",
    "    st.markdown(f'''__Publication Location:__ {doc_detailsspd.PUBLICATION_LOCATION.iloc[0]}''')\n",
    "    st.markdown(f'''__Morbidities Mentioned:__ {doc_details.select(F.array_to_string(F.col('MORBIDITIES'),F.lit(','))).collect()[0][0]}''')\n",
    "    \n",
    "\n",
    "# New Section \n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as snow_funcs\n",
    "\n",
    "import pypdfium2 as pdfium\n",
    "from datetime import datetime\n",
    "\n",
    "# Write directly to the app\n",
    "\n",
    "\n",
    "doc_ai_context = \"DEFAULT_DATABASE.DOCUMENT_AI\"\n",
    "doc_ai_source_table = \"RESEARCH_PAPERS_ALL_DATA\"\n",
    "doc_ai_source_verify_table = \"RESEARCH_PAPERS_ALL_DATA\"\n",
    "doc_ai_doc_stage = \"RESEARCH_PAPERS\"\n",
    "\n",
    "# Dict that has the name of the columns that needs to be verified, it has the column name of the column \n",
    "# with value and column with the score\n",
    "value_dict = {\n",
    "    \"OPERATOR_VALUE\": {\n",
    "        \"VAL_COL\": \"OPERATOR_VALUE\",\n",
    "        \"SCORE_COL\": \"OPERATOR_SCORE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# The minimum score needed to not be verified\n",
    "threshold_score = 0.5\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "# Function to generate filter to only get the rows that are missing values or have a score below the threshold\n",
    "def generate_filter(col_dict:dict,  score_val:float): #score_cols:list, score_val:float, val_cols:list):\n",
    "    \n",
    "    filter_exp = ''\n",
    "\n",
    "    # For each column\n",
    "    for col in col_dict:\n",
    "        # Create the filter on score threashold or missing value\n",
    "        if len(filter_exp) > 0:\n",
    "                filter_exp += ' OR '\n",
    "        filter_exp += f'(({col_dict[col][\"SCORE_COL\"]} <= {score_val} ) OR ({col_dict[col][\"VAL_COL\"]} IS NULL))'\n",
    "\n",
    "    if len(filter_exp) > 0:\n",
    "       filter_exp = f'({filter_exp}) AND ' \n",
    "    \n",
    "    # Filter out documents already verified\n",
    "    filter_exp  += 'verification_date is null'\n",
    "    return filter_exp\n",
    "\n",
    "# Generates a column list for counting the number of documents that is missing values or a score less that the threashold\n",
    "# by each column\n",
    "def count_missing_select(col_dict:dict, score_val:float):\n",
    "    select_list = []\n",
    "\n",
    "    for col in col_dict:\n",
    "        col_exp = (snow_funcs.sum(\n",
    "                          snow_funcs.iff(\n",
    "                                    (\n",
    "                                        (snow_funcs.col(col_dict[col][\"VAL_COL\"]).is_null())\n",
    "                                        | \n",
    "                                        (snow_funcs.col(col_dict[col][\"SCORE_COL\"]) <= score_val)\n",
    "                                    ), 1,0\n",
    "                              )\n",
    "                      ).as_(col)\n",
    "                )\n",
    "        select_list.append(col_exp)\n",
    "        \n",
    "    return select_list\n",
    "\n",
    "# Function to display a pdf page\n",
    "def display_pdf_page():\n",
    "    pdf = st.session_state['pdf_doc']\n",
    "    page = pdf[st.session_state['pdf_page']]\n",
    "            \n",
    "    bitmap = page.render(\n",
    "                    scale = 8, \n",
    "                    rotation = 0,\n",
    "            )\n",
    "    pil_image = bitmap.to_pil()\n",
    "    st.image(pil_image)\n",
    "\n",
    "# Function to move to the next PDF page\n",
    "def next_pdf_page():\n",
    "    if st.session_state.pdf_page + 1 >= len(st.session_state['pdf_doc']):\n",
    "        st.session_state.pdf_page = 0\n",
    "    else:\n",
    "        st.session_state.pdf_page += 1\n",
    "\n",
    "# Function to move to the previous PDF page\n",
    "def previous_pdf_page():\n",
    "    if st.session_state.pdf_page > 0:\n",
    "        st.session_state.pdf_page -= 1\n",
    "\n",
    "# Function to get the name of all documents that need verification\n",
    "def get_documents(doc_df):\n",
    "    \n",
    "    lst_docs = [dbRow[0] for dbRow in doc_df.collect()]\n",
    "    # Add a default None value\n",
    "    lst_docs.insert(0, None)\n",
    "    return lst_docs\n",
    "\n",
    "# MAIN\n",
    "\n",
    "# Get the table with all documents with extracted values\n",
    "df_agreements = session.table(f\"{doc_ai_context}.{doc_ai_source_table}\")\n",
    "\n",
    "# Get the documents we already gave verified\n",
    "df_validated_docs = session.table(f\"{doc_ai_context}.{doc_ai_source_verify_table}\")\n",
    "\n",
    "# Join\n",
    "df_all_docs = df_agreements.join(df_validated_docs,on='RELATIVE_PATH', how='left', lsuffix = '_L', rsuffix = '_R')\n",
    "\n",
    "# Filter out all document that has missing values of score below the threasholds\n",
    "validate_filter = generate_filter(value_dict, threshold_score)\n",
    "df_validate_docs = df_all_docs.filter(validate_filter)\n",
    "#col1, col2 = st.columns(2)\n",
    "#col1.metric(label=\"Total Documents\", value=df_agreements.count())\n",
    "#col2.metric(label=\"Documents Needing Validation\", value=df_validate_docs.count())\n",
    "\n",
    "# Get the number of documents by value that needs verifying\n",
    "#select_list = count_missing_select(value_dict, threshold_score)\n",
    "#df_verify_counts = df_validate_docs.select(select_list)\n",
    "#verify_cols = df_verify_counts.columns\n",
    "\n",
    "#st.subheader(\"Number of documents needing validation by extraction value\")\n",
    "#st.bar_chart(data=df_verify_counts.unpivot(\"needs_verify\", \"check_col\", verify_cols), x=\"CHECK_COL\", y=\"NEEDS_VERIFY\")\n",
    "\n",
    "# Verification section\n",
    "st.divider()\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    st.markdown('#### RAW PDF STORED IN FILE STORE')\n",
    "    with st.container():\n",
    "        # If we have selected a document\n",
    "        if file_id:        \n",
    "        # Display the extracted values\n",
    "            df_doc = df_validate_docs.filter(snow_funcs.col(\"FILE_NAME\") == file_id)\n",
    "            if 'pdf_page' not in st.session_state:\n",
    "                st.session_state['pdf_page'] = 0\n",
    "            if 'pdf_url' not in st.session_state:\n",
    "                st.session_state['pdf_url'] = file_id    \n",
    "            if 'pdf_doc' not in st.session_state or st.session_state['pdf_url'] != file_id:\n",
    "                pdf_stream = session.file.get_stream(f\"@{doc_ai_context}.{doc_ai_doc_stage}/{file_id}\")\n",
    "                pdf = pdfium.PdfDocument(pdf_stream)\n",
    "                st.session_state['pdf_doc'] = pdf\n",
    "                st.session_state['pdf_url'] = file_id\n",
    "                st.session_state['pdf_page'] = 0\n",
    "                \n",
    "            nav_col1, nav_col2, nav_col3 = st.columns(3)\n",
    "            with nav_col1:\n",
    "                if st.button(\"⏮️ Previous\", on_click=previous_pdf_page):\n",
    "                    pass    \n",
    "                with nav_col2:\n",
    "                    st.write(f\"page {st.session_state['pdf_page'] +1} of {len(st.session_state['pdf_doc'])} pages\")\n",
    "                with nav_col3:\n",
    "                    if st.button(\"Next ⏭️\", on_click=next_pdf_page):\n",
    "                        pass\n",
    "        \n",
    "    \n",
    "    \n",
    "            display_pdf_page()\n",
    "    with col2:\n",
    "        st.markdown('#### EXTRACTED TEXT FROM PDFS')\n",
    "        with st.container(height=800):\n",
    "            st.markdown(doc_detailsspd.CONTENT.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a75a2-872e-4042-85ee-2edc8a5f29e5",
   "metadata": {
    "collapsed": false,
    "name": "summarise"
   },
   "source": [
    "### CORTEX SUMMARISE\n",
    "You will notice that some of these reports are very long.  Let's use Cortex Summarize to summarise them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c4d08-ded9-4699-813e-5a18705d33d4",
   "metadata": {
    "language": "sql",
    "name": "summarise_results"
   },
   "outputs": [],
   "source": [
    "SELECT TITLE_OF_PAPER, SNOWFLAKE.CORTEX.SUMMARIZE(CONTENT) SUMMARY FROM DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA limit 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4623cc-875d-4587-90d7-3cc846574454",
   "metadata": {
    "collapsed": false,
    "name": "title_summary_all"
   },
   "source": [
    "You will now create a table which combines all the document meta data with a summary for each research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f70d3-81f8-4706-a439-eecb8a6695f8",
   "metadata": {
    "language": "sql",
    "name": "summarise_and_retain_meta_data_from_doc_ai"
   },
   "outputs": [],
   "source": [
    "-- Formatting the summarised reports to use with the search service\n",
    "\n",
    "CREATE OR REPLACE TABLE DEFAULT_SCHEMA.SUMMARISED_RESEARCH_PAPERS AS \n",
    "\n",
    "\n",
    "SELECT \n",
    "RELATIVE_PATH,\n",
    "AUTHORS,\n",
    "TITLE_OF_PAPER,\n",
    "DATE_PUBLICATION,\n",
    "PUBLICATION_LOCATION,\n",
    "'SUMMARY' CONTENT, \n",
    "SPLIT_PART(RELATIVE_PATH,'/',1)::text DOCUMENT,\n",
    "SUMMARY TEXT\n",
    "\n",
    "FROM\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SELECT * EXCLUDE CONTENT,SNOWFLAKE.CORTEX.SUMMARIZE(CONTENT) SUMMARY FROM DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA);\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.SUMMARISED_RESEARCH_PAPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c324c88-d67a-48f8-a377-a3385970acca",
   "metadata": {
    "language": "sql",
    "name": "summary_table"
   },
   "outputs": [],
   "source": [
    "select * from DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6b87a-7119-4dd1-964a-5b9347d99b2c",
   "metadata": {
    "collapsed": false,
    "name": "details"
   },
   "source": [
    "Allthough a high level summary is now created, the user may need to drill into the details.  These details could reside in documents of many pages.  We will now 'chunk' each paper into bite size pieces.  This will effectively create muliple rows of data per chunk.  The function **SPLIT_TEXT_RECURSIVE_CHARACTER** is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dccb935-d2c1-4d74-a299-3ab9f4ac43fa",
   "metadata": {
    "language": "sql",
    "name": "chunked_data_together"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DEFAULT_SCHEMA.DETAILED_RESEARCH_PAPERS_CHUNKED_AND_SUMMARISED AS \n",
    "\n",
    "SELECT * FROM\n",
    "(\n",
    "SELECT \n",
    "RELATIVE_PATH,\n",
    "AUTHORS,\n",
    "TITLE_OF_PAPER,\n",
    "DATE_PUBLICATION,\n",
    "PUBLICATION_LOCATION,\n",
    "'DETAILED' CONTENT, \n",
    "SPLIT_PART(RELATIVE_PATH,'/',1)::text DOCUMENT,\n",
    "VALUE::TEXT TEXT\n",
    "\n",
    "\n",
    "FROM\n",
    "\n",
    "\n",
    "DEFAULT_SCHEMA.RESEARCH_PAPERS_ALL_DATA,\n",
    "\n",
    "LATERAL FLATTEN(SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(CONTENT,'none',600,50,['\\n\\n', ' '])))\n",
    "\n",
    "UNION \n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.SUMMARISED_RESEARCH_PAPERS;\n",
    "\n",
    "SELECT * FROM DEFAULT_SCHEMA.DETAILED_RESEARCH_PAPERS_CHUNKED_AND_SUMMARISED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533437b7-3c42-4d4b-9412-9e1e88de5f7e",
   "metadata": {
    "collapsed": false,
    "name": "search_Service"
   },
   "source": [
    "## CREATE A SEARCH SERVICE\n",
    "Now you will create a search service in order for a **Cortex Agent** can search through and provide answers to your health related questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe7408-9182-4d59-97e3-c011d589f860",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "search_service_creation"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE  CORTEX SEARCH SERVICE DEFAULT_SCHEMA.RESEARCH_PAPERS\n",
    "  ON TEXT\n",
    "  ATTRIBUTES AUTHORS,TITLE_OF_PAPER,DATE_PUBLICATION,CONTENT,DOCUMENT\n",
    "  WAREHOUSE = DEFAULT_WH\n",
    "  TARGET_LAG = '1 hour'\n",
    "  COMMENT = 'SEARCH SERVICE FOR RESEARCH_PAPERS'\n",
    "  AS SELECT * FROM DEFAULT_SCHEMA.DETAILED_RESEARCH_PAPERS_CHUNKED_AND_SUMMARISED;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85914567-b5a9-48af-abce-888eed5bd4b0",
   "metadata": {
    "collapsed": false,
    "name": "SIMULATE_DATA"
   },
   "source": [
    "So we have now created a search service to enable an agent to answer questions regarding the research papers.  The Structured datasets can also be used in the cortex agent - but before we can do this, we will need to bring this data together through a **semantic model**.  Structured data will not just contain the structured results from document ai - but will  contain more traditionally created datasets.  This lab contains a synthetic dataset summarising the general health of millions of people located in Great Britain. This will be a good place to start.  But it will be great to synthesise more information about sample patients such as blood pressure history and any medical reports.  Let's tryout **Cortex Playground** to see the capabilities of the built in LLMs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "9025212424216",
   "authorName": "USER",
   "lastEditTime": 1744663681351,
   "notebookId": "mle3gx4r42vorf6kb5aa",
   "sessionId": "1765ffe7-d2bc-44c1-a2df-e8275520b88a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
