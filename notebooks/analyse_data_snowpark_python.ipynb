{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75ecdf4-2f5d-48ed-8527-0bcad428bc0c",
   "metadata": {
    "collapsed": false,
    "name": "title",
    "resultHeight": 162
   },
   "source": [
    "# Python for Data Analysts \n",
    "### Analyse and visualise data with Snowpark for Python and Streamlit\n",
    "\n",
    "\n",
    "Here is an introduction to some handy snowpark techniques and how you can use it to help with visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d3ac4-4ef9-420b-8af3-60b495460b6d",
   "metadata": {
    "collapsed": false,
    "name": "what_is_snowpwrk",
    "resultHeight": 592
   },
   "source": [
    "### What is Snowpark\n",
    "\n",
    "![df](https://storage.googleapis.com/gae-wp-itsol-prd.appspot.com/1/Snowflake-Blog_vol.014_1.png)\n",
    "\n",
    "Snowpark is a series of APIs which allow you to transform, model and perform operations on your data using a Language of choice.  Natively, Snowpark supports Python, Scala, Java and SQL.  However, if you would like to leverage an additional language, you can leverage Snowpark Container Services.  This tutorial will get you familariar with Snowpark dataframes for python.\n",
    "\n",
    "https://docs.snowflake.com/en/developer-guide/snowpark/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde22b4-bd59-451d-9888-cfdaa904067d",
   "metadata": {
    "collapsed": false,
    "name": "intro",
    "resultHeight": 315
   },
   "source": [
    "#### Importing the relevant Libraries\n",
    "\n",
    "We will import the python libraries.  If you wish to use a note book outside of snowflake workbooks, you can simply **pip install** them using the following link:\n",
    "\n",
    "https://pypi.org/project/snowflake-snowpark-python/\n",
    "\n",
    "The main difference between using snowpark outside of the snowflake ui (Snowsight) and using within Snowsight is how you connect to the data. If you leverage Snowflake data in your own interface such as jupyter and want to use snowpark you will need to set up a session which requires credentials to be passed. How this is achieved is documented in the pypi documentation.  In this session, will be using the connection which is dynamically provided by the session.  The session has already been authenticated when you logged into snowflake so will pass on any of the relevent security privileges and constraints.  The other difference is that in order to render the results, the notebook needs to leverage streamlit which is built into snowflake.  Whether you choose to utilise Snowpark with the provided UI or your own, Snowpark still provides the same governence and powerful compute provided by Snowflake, which allows for fast secure analytics.\n",
    "\n",
    "So without going any further, run the next cell to load the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "import_libaries",
    "resultHeight": 88
   },
   "outputs": [],
   "source": [
    "# Import streamlit\n",
    "import streamlit as st\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "st.markdown('#### Hello, welcome to snowpark')\n",
    "st.markdown('You will see that we have imported **get_active_session** from snowpark and have also imported streamlit to render any results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1facbdc-5068-44f4-94c3-f7fd7fb70905",
   "metadata": {
    "collapsed": false,
    "name": "title_write_sql",
    "resultHeight": 114
   },
   "source": [
    "#### Writing SQL\n",
    "If you want to write sql in snowpark you can do this by using **session.sql**.   You can create any query accross any schema / database / view in snowflake using this syntax.  Below, I have created a query and saved it as a dataframe.  I then called back the dataframe to retrieve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d87d26-3637-44c1-b41f-1aff85d2a08b",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql",
    "resultHeight": 251
   },
   "outputs": [],
   "source": [
    "SELECT * FROM DEFAULT_SCHEMA.SYNTHETIC_POPULATION LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ed175-4578-4961-92a4-794342514b94",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "create_basic_sql",
    "resultHeight": 251
   },
   "outputs": [],
   "source": [
    "dataframe = session.sql('select * from DEFAULT_SCHEMA.SYNTHETIC_POPULATION')\n",
    "\n",
    "dataframe.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887e6e9-4645-4c0d-949b-134ed97e1773",
   "metadata": {
    "collapsed": false,
    "name": "heading_distinct"
   },
   "source": [
    "Any field in the dataframe, you can view the distinct values.  Below you will see the distinct values for marital status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf99e95-0f3a-4e22-8f39-f5777420fe29",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell15",
    "resultHeight": 321
   },
   "outputs": [],
   "source": [
    "dataframe.select('MARITAL_STATUS').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3bffa-0463-4d57-995b-56931a1ddf1f",
   "metadata": {
    "collapsed": false,
    "name": "Exercise_1"
   },
   "source": [
    "### Exercise\n",
    "Create a list of distinct values for the field **OCCUPATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c437e-a8fa-45e0-bed2-2b53727379cf",
   "metadata": {
    "language": "python",
    "name": "complete_exercise_1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41916236-6a9e-4c3f-af95-77235d59b28b",
   "metadata": {
    "collapsed": false,
    "name": "export_title",
    "resultHeight": 88
   },
   "source": [
    "#### Export out to Pandas\n",
    "Any dataframe you make in snowpark can be exported out as a pandas dataframe. You simply add **to_pandas()** at the end of the dataframe and you very quickly get a pandas dataframe\n",
    "\n",
    "\n",
    "**What is Pandas??**\n",
    "Pandas is a Python library used for working with data sets.  It has functions for analyzing, cleaning, exploring, and manipulating data.  Below you will see the snowpark dataframe converted to Pandas, then you will see the data types associated with each column in the dataset.  \n",
    "\n",
    "From our perspective, pandas is useful for visualisation puroposes.  Many of the python visualisation libraries work with pandas.  The down side of pandas is its not fast in terms of processing.  For Large scale multi threaded data engineering, work with Snowpark dataframes, as it will leverage parellel processing and all the standard functionality that Snowflake offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bce29a-f81c-4457-9f07-bdcc3eb6980a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "export_out_to_pandas",
    "resultHeight": 722
   },
   "outputs": [],
   "source": [
    "dataframepd = dataframe.limit(100).to_pandas()\n",
    "\n",
    "st.markdown('##### This is a dataframe converted to pandas')\n",
    "st.write(dataframepd.head())\n",
    "st.markdown('##### These are the pandas datatypes')\n",
    "st.write(dataframepd.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce0c32-20e1-4c20-8639-10bc361b51cc",
   "metadata": {
    "collapsed": false,
    "name": "desc_pandas",
    "resultHeight": 67
   },
   "source": [
    "Likewise, you might want to load a pandas dataframe and load it as a snowpark dataframe.  in reality this could have been what was once a csv file which you loaded - or simply to re-import data that was originated from Snowflake back into a snowpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd0394-9f3c-4b10-b466-f4ac2dcde1eb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "create_dframe_from_pandas",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "dataframe_2 = session.createDataFrame(dataframepd)\n",
    "dataframe_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54cbe18-582a-48fa-966d-9ea653f191dd",
   "metadata": {
    "collapsed": false,
    "name": "title_columns",
    "resultHeight": 41
   },
   "source": [
    "Now we its back as a snowpark dataframe, lets inspect the columns along with their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcce48a-80d6-4fd2-92c3-554dfbc0d4a4",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "columns",
    "resultHeight": 1581
   },
   "outputs": [],
   "source": [
    "dataframe.print_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d03d5-75df-4c99-aff9-c0566199fab8",
   "metadata": {
    "collapsed": false,
    "name": "desc_count_table",
    "resultHeight": 67
   },
   "source": [
    "At the beginning, we created a dataframe using SQL.   We can also create a dataframe by simply specifying the table name. **Note**  You will see that the table holds over 64 million records.  We worked this out easily with the **count()** feature.  As we know there are a lot of rows, we have limited the result set by adding **limit(5)** to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e85a54-806f-436c-ae2d-e063d587e79c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "count_dataframe",
    "resultHeight": 278
   },
   "outputs": [],
   "source": [
    "dataframe = session.table('DEFAULT_SCHEMA.SYNTHETIC_POPULATION')\n",
    "\n",
    "st.write(dataframe.count())\n",
    "st.write(dataframe.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74126ed-96a2-479e-a9be-f163f703d8d7",
   "metadata": {
    "collapsed": false,
    "name": "rename_columns"
   },
   "source": [
    "You might have noticed that there is a column called 'MULTIPLE_MOBILITIES' - this is meant to say 'MULTIPLE_MORBIDITIES'.  Its simple to rename any column - look at how we do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5301c0-5de2-4a1a-80fe-325451c01439",
   "metadata": {
    "language": "python",
    "name": "rename_column"
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe.with_column_renamed('MULTIPLE_MOBILITIES','MULTIPLE_MORBIDITIES')\n",
    "dataframe.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9daa42-9b25-4c76-99fc-496c8eb34c97",
   "metadata": {
    "collapsed": false,
    "name": "selecting_columns"
   },
   "source": [
    "It is easy to select the columns you want in a dataframe by using **select** after the dataframe name. as we have over 30million columns, we will limit the results to the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9447a-90f5-4240-8761-fc5988a634fa",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "dataframe.select('FIRST_NAME','LAST_NAME','PRACTICE_CODE','PRACTICE_NAME').limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4072a-773b-4dca-a12e-66db5c9fa0ae",
   "metadata": {
    "collapsed": false,
    "name": "exercise_select_columns"
   },
   "source": [
    "Use the select construct to select some columns of your choice.  In python, I have added the dataframe.columns command to give you an idea of the columns you may want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8497d96-cf6f-492f-8c27-d0e66cda0cee",
   "metadata": {
    "language": "python",
    "name": "select_columns_practice"
   },
   "outputs": [],
   "source": [
    "#here is the command to reveal the columns\n",
    "dataframe.columns\n",
    "\n",
    "### below, write the correct python to select 5 columns from the dataframe, and limit the rows to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbaf9f-e5fb-4bd7-9ddf-5a926099f706",
   "metadata": {
    "collapsed": false,
    "name": "heading_add_functions",
    "resultHeight": 88
   },
   "source": [
    "#### Using snowflake functions in Python.\n",
    "Any function whether built in or custom functions can be used in python dataframes. Its easier to import them so they are accessible by using the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea9953-e5f5-424e-a998-d05ad5326a17",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "add_functions",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2afb3-0b8f-4109-abe1-3a6ff538cffd",
   "metadata": {
    "collapsed": false,
    "name": "title_renamed",
    "resultHeight": 283
   },
   "source": [
    "#### Let's add a new column for age.  This can be calculated from the data of birth. \n",
    "\n",
    "Firstly, you will use the **with_column** construct to create a new column, within this the new column is calculated from an **expression**.  This expression uses the built in **datadiff** function.  All SQL functions available in Snowflake SQL are also available in Snowpark for Python.\n",
    "\n",
    "Below we are using the datedif function to calculate the citizens age in years.  after this i quickly displayed the results by doing the following:\n",
    "- selecting the first 5 rows and selecting a sub set of columns with include age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c185191-3cad-4b47-961c-53d6b586ffd3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "view_data_ages",
    "resultHeight": 1120
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe.with_column('AGE',datediff('year',col('DATE_OF_BIRTH'),current_date()))\n",
    "\n",
    "st.dataframe(dataframe.select('NHS_NUMBER','FIRST_NAME','LAST_NAME','AGE').limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fff86-cefc-4d19-84c8-19564a7a9030",
   "metadata": {
    "collapsed": false,
    "name": "grouping_sorting"
   },
   "source": [
    "Now we have selected the right columns and included an additional column that includes the age, we will group the ages together with a groubin b by age, then sort the grouped dataframe by age.\n",
    "\n",
    "- grouping the ages with a count and sum to view total morbidities and the count of all people that have that age.\n",
    "- in the view by age, we have sorted by age with the 'sort' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c8325-1c65-4567-9398-bdedc8407b9e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "dataframe.group_by('AGE').agg(sum('MULTIPLE_MORBIDITIES').alias('\"Total Morbidities\"'),count('*').alias('\"Total Citizens\"')).sort('AGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1582e-0e0d-496f-a080-2fdbc4e3fa2f",
   "metadata": {
    "collapsed": false,
    "name": "filtering_under_1_yr_olds",
    "resultHeight": 67
   },
   "source": [
    "Lets filter some of the data from the dataset. A new dataframe is created to ov  I have decided not to have under 5 yr olds in my analysis - therefore I will filter them out.  Next, the result is saved to a new table and finally we will view a sample of the data under 5.  Notice we are using select again to preview only the columns we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af06f1e-2cb3-4411-8ffc-9294a62ecddc",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "drop_empty_rows",
    "resultHeight": 95
   },
   "outputs": [],
   "source": [
    "dataframe_u5 = dataframe.filter(col('AGE')>=5)\n",
    "dataframe_u5.write.mode('overwrite').save_as_table('DEFAULT_SCHEMA.\"Population Health Synthetic Data_over_5\"')\n",
    "dataframe.filter(col('AGE')<5).limit(10).select('NHS_NUMBER','FIRST_NAME','LAST_NAME','AGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a25ce-bc8f-4d72-937c-cfbce51ec41b",
   "metadata": {
    "collapsed": false,
    "name": "title_distincts",
    "resultHeight": 92
   },
   "source": [
    "Snowpark dataframes make it easy to create lots of components for different purposes.  You would have previously used the distinct function to view all occupation types.  You could use this to populate a select box for dynamic filtering purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba12fb-c728-426c-a954-3504163e2d79",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "distinct",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "dataframe = session.table('DEFAULT_SCHEMA.\"Population Health Synthetic Data_over_5\"')\n",
    "occupations = dataframe.select('OCCUPATION').distinct()\n",
    "st.selectbox('Choose Your Occupation:',occupations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec65e72-41a0-46f2-a427-c97d38c3c72d",
   "metadata": {
    "collapsed": false,
    "name": "title_filter",
    "resultHeight": 92
   },
   "source": [
    "Lets see this new dataframe in action.  The select box is given a name called filter.  This name is used as a variable to filter the occupation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b21da6-11f9-4e13-9ae0-c5d38026f0cb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "filter_occupations",
    "resultHeight": 319
   },
   "outputs": [],
   "source": [
    "filter = st.selectbox('Choose Occupation:',occupations)\n",
    "\n",
    "filtered_df = dataframe.filter(col('OCCUPATION')==filter)\n",
    "filtered_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0646c0-f8d2-4458-acb6-0af9a56f4de5",
   "metadata": {
    "collapsed": false,
    "name": "title_group_by",
    "resultHeight": 47
   },
   "source": [
    "#### Now we have selected a few results, lets group the filtered dataset by marital status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c738168-ab8b-447d-a53a-f698adc3b5a6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "group_by",
    "resultHeight": 251
   },
   "outputs": [],
   "source": [
    "filtered_df.group_by('MARITAL_STATUS').agg(sum('\"MULTIPLE_MORBIDITIES\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e781b2b-cdbd-4e54-9dee-9002f51164c3",
   "metadata": {
    "collapsed": false,
    "name": "title_add_more_columns",
    "resultHeight": 41
   },
   "source": [
    "We will use the aggregation construct to summarise more figures based on the marital status grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc401c1-c324-4725-be06-2db4af3a7901",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "add_column_aggregates",
    "resultHeight": 251
   },
   "outputs": [],
   "source": [
    "filtered_df.group_by('MARITAL_STATUS').agg(sum('MULTIPLE_MORBIDITIES'),\n",
    "                                           sum('CANCER'),\n",
    "                                            sum('DIABETES'),\n",
    "                                            sum('COPD'),\n",
    "                                            sum('ASTHMA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa67bd-1cc4-4542-8dc7-6e3e2064049f",
   "metadata": {
    "collapsed": false,
    "name": "rename_columns_alias",
    "resultHeight": 41
   },
   "source": [
    "Next, let's rename the columns to make them clearer.  This is by using the 'alias' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb052a-d37f-4b85-8da1-ede828712813",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "group_by_marital_status",
    "resultHeight": 286
   },
   "outputs": [],
   "source": [
    "dataframe.group_by('MARITAL_STATUS').agg(sum('MULTIPLE_MORBIDITIES').alias('\"Multiple Morbidities\"'),\n",
    "                                           sum('CANCER').alias('\"Cancer\"'),\n",
    "                                            sum('DIABETES').alias('\"Diabetes\"'),\n",
    "                                            sum('COPD').alias('\"COPD\"'),\n",
    "                                            sum('ASTHMA').alias('\"Asthma\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181db2c0-5438-4060-92e3-7a0a7c4a169f",
   "metadata": {
    "collapsed": false,
    "name": "title_pivot",
    "resultHeight": 41
   },
   "source": [
    "It is easy to effectively view comparisons of results side by side.  But what if you would like to do it using data accross multiple rows.  This is where the **pivot** function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049de378-9cbe-4b5e-81ff-7b4d7aee520e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "pivot",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "pivot_selection = dataframe.select('ICB22NM','PRACTICE_NAME','MARITAL_STATUS','CANCER')\n",
    "\n",
    "fields = dataframe.select('MARITAL_STATUS').distinct()\n",
    "\n",
    "pivotted = pivot_selection.pivot(col('MARITAL_STATUS'), fields).sum('\"CANCER\"')\n",
    "\n",
    "pivotted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0cc426-f427-4016-8341-f84d165aa82c",
   "metadata": {
    "collapsed": false,
    "name": "title_mass_cleaning",
    "resultHeight": 114
   },
   "source": [
    "#### Mass cleaning of column names\n",
    "As Snowpark is programmable you can leverage python functions for the cleaning of multiple column names.  In this case we are converting to upper case.  We have imported the Dataframe Object in order to make changes to the structure of the dataframe.\n",
    "\n",
    "Below is using a **python function** make changes to the column name and remove undesired characters.  All python functions start are defined as **def**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2dc3c-3846-4cfc-a561-55139b2ba5cf",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "function_clean_columns",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import DataFrame\n",
    "\n",
    "def clean_col_names(df: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "        removes annoying single & double quotes in the column names\n",
    "        also makes all characters in the column names lowercase\n",
    "    '''\n",
    "    df_res = df\n",
    "    for col in df.columns:\n",
    "        df_res = df_res.with_column_renamed(col, col.upper().replace('\\'', '').replace('\"', ''))\n",
    "        \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d873f-13b1-4a7c-8b43-c83dd3be75df",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "we will run the function that has just been defined and call the resulting dataframe **pivotted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a45cf-8565-4a00-9d82-cab641639db3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "clean_pivotted_columns",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "pivotted = clean_col_names(pivotted)\n",
    "pivotted.limit(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbacf5-28e1-443f-8065-85f838a24ae6",
   "metadata": {
    "collapsed": false,
    "name": "put_data_in_chart",
    "resultHeight": 114
   },
   "source": [
    "#### Putting data in a chart\n",
    "If you want to move the data into a chart you need to convert the result set to pandas first by using the **to_pandas()** feature.  Only send what you need to visualise after doing filtering/transformations in snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08411d1e-c99c-4c93-92d1-9d2b68b3c96c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "putting_data_in_scatter",
    "resultHeight": 372
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "\n",
    "\n",
    "c = (\n",
    "   alt.Chart(pivotted.to_pandas())\n",
    "   .mark_circle()\n",
    "   .encode(x=\" MARRIED OR IN A SAME SEX CIVIL PARTNERSHIP\",\n",
    "           y=\"SINGLE\", \n",
    "           size=\"SEPERATED (BUT STILL LEGALLY MARRIED OR STILL LEGALLY IN A SAME-SEX CIVIL PARTNERSHIP)\", \n",
    "           color=\"<16 YEARS OLD THEREFORE INELIGIBLE TO MARRY\", tooltip=[\"PRACTICE_NAME\"])\n",
    ")\n",
    "\n",
    "st.altair_chart(c, use_container_width=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623ac2f-4636-441b-895e-ac5eed009c1f",
   "metadata": {
    "collapsed": false,
    "name": "title_add_interaction",
    "resultHeight": 41
   },
   "source": [
    "Below we are providing interactivity on both the ICB and the type of morbidity by parameterising the snowpark dataframe during the transformation process.  The last step is to convert to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e41f6a-eb54-4e0c-b52c-fc8eb9bfa4a6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "chart_with_filter",
    "resultHeight": 581
   },
   "outputs": [],
   "source": [
    "distinct_icb = dataframe.select('ICB22NM').distinct()\n",
    "\n",
    "s_icb = st.selectbox('choose icb:',distinct_icb)\n",
    "\n",
    "s_morbidities = st.selectbox('select morbidity',['ASTHMA','COPD','DIABETES','CANCER','MULTIPLE_MORBIDITIES'])\n",
    "pivot_selection = dataframe.select('ICB22NM','PRACTICE_NAME','MARITAL_STATUS',f'{s_morbidities}')\n",
    "#pivot_selection = filtered_df.select('ICB22NM','PRACTICE_NAME','MARITAL_STATUS',)\n",
    "\n",
    "pivotted = pivot_selection.pivot(col('MARITAL_STATUS'), fields).sum(f'\"{s_morbidities}\"')\n",
    "pivotted = clean_col_names(pivotted)\n",
    "\n",
    "\n",
    "pivottedpd = pivotted.filter(col('ICB22NM')==s_icb).to_pandas()\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "c = (\n",
    "   alt.Chart(pivottedpd)\n",
    "   .mark_circle()\n",
    "   .encode(x=\" MARRIED OR IN A SAME SEX CIVIL PARTNERSHIP\",\n",
    "           y=\"SINGLE\", \n",
    "           size=\"SEPERATED (BUT STILL LEGALLY MARRIED OR STILL LEGALLY IN A SAME-SEX CIVIL PARTNERSHIP)\", \n",
    "           color=\"<16 YEARS OLD THEREFORE INELIGIBLE TO MARRY\", tooltip=[\"PRACTICE_NAME\"])\n",
    ")\n",
    "\n",
    "st.markdown('Patients by Marital Status')\n",
    "st.altair_chart(c, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f7762-dd94-4625-8838-1fcb403441b9",
   "metadata": {
    "collapsed": false,
    "name": "more_filtering",
    "resultHeight": 88
   },
   "source": [
    "#### More on Filtering\n",
    "Below we can see data which is filtering everyone betweeen the age of 11 and 16 years of age.  This is using the **between** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458c7a3-98d0-4505-b8f0-d5a6739376aa",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "filter_between",
    "resultHeight": 505
   },
   "outputs": [],
   "source": [
    "st.markdown('Filter Results based on the age being between 11 and 16')\n",
    "st.write(dataframe.filter(col('Age').between(11,16)).sample(0.1).limit(1000))\n",
    "st.markdown('NB - we are limiting on 1000 records based on a 10% random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8c059-a481-4b0c-948b-55f7b59d7c0d",
   "metadata": {
    "collapsed": false,
    "name": "heading_filter_practices",
    "resultHeight": 41
   },
   "source": [
    "Here we are filtering on practices that begin with Y. For this, its easy to use the **.like** feature where the **%** is used as a wild card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d815bf5-0ff9-431d-9fe3-9fb6d9a5e1eb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "filter_like",
    "resultHeight": 505
   },
   "outputs": [],
   "source": [
    "st.markdown('Filter Results based on Practice Code starting with Y')\n",
    "st.write(dataframe.filter(col('PRACTICE_CODE').like('Y%')).sample(0.1).limit(1000))\n",
    "st.markdown('NB - we are limiting on 1000 records based on a 10% random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33848d1-54ba-4a9c-8eae-fe4acb20718c",
   "metadata": {
    "collapsed": false,
    "name": "filter_title_multiple",
    "resultHeight": 41
   },
   "source": [
    "Here, we are filtering using multple filters - looking at the healthcode which is less than or equal to 2 and also all males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1e7ff-a1a9-4377-b648-492a5ff09f21",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "multiple_filters",
    "resultHeight": 505
   },
   "outputs": [],
   "source": [
    "st.markdown('Filter Results based on Health Code being less than 2 and Sex being Male')\n",
    "st.write(dataframe.filter((col('GENERAL_HEALTH_CODE') <=2)& (col('SEX') =='Male')).sample(0.1).limit(1000))\n",
    "st.markdown('NB - we are limiting on 1000 records based on a 10% random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538181ff-a55d-4ac2-bc74-1432e126bf9d",
   "metadata": {
    "collapsed": false,
    "name": "title_buckets",
    "resultHeight": 47
   },
   "source": [
    "#### Putting Data into Buckets.  There are many ways to group data together into buckets in order to dp specific types of analysis.  Below are 3 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced69ff-0faa-4bc4-b7f9-f6ce0221315c",
   "metadata": {
    "collapsed": false,
    "name": "heading_bucketing",
    "resultHeight": 41
   },
   "source": [
    "Bucketing by timeslices - so bacically grouping the data of birth into 3 month intervals.  Snowflake has powerful time series analysis and this can be leveraged though SQL as well as snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a155cb2-a63f-414a-82fd-9b022ee16f19",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Bucketting",
    "resultHeight": 414
   },
   "outputs": [],
   "source": [
    "time_slice = dataframe.select('DATE_OF_BIRTH',call_function('time_slice',col('DATE_OF_BIRTH'),3,'MONTH','END').alias('month_interval'),'MULTIPLE_MORBIDITIES')\n",
    "time_slice = time_slice.group_by('month_interval').agg(sum('MULTIPLE_MORBIDITIES').alias('MULTIPLE_MORBIDITIES'))\n",
    "st.markdown('Date of Birth vs multiple morbidities')\n",
    "st.line_chart(time_slice,y='MULTIPLE_MORBIDITIES',x='MONTH_INTERVAL')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baaafd9-fce2-4d1b-b397-6a6a8b6546c9",
   "metadata": {
    "collapsed": false,
    "name": "title_width_bucket",
    "resultHeight": 41
   },
   "source": [
    "The Second Bucket is a width Bucket.  This time we will bucket on the actual age and will build a bar chart on this.    **ST_BAR_CHART** is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f8c34-6cb4-4882-9d52-1cd36f265f6f",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "width_bucket",
    "resultHeight": 372
   },
   "outputs": [],
   "source": [
    "age_bucket = dataframe.select('MULTIPLE_MORBIDITIES',\n",
    "                                 call_function('WIDTH_BUCKET',\n",
    "                                               col('AGE'),0,110,6).alias('AGE_BUCKET'))\\\n",
    ".group_by('AGE_BUCKET').agg(sum('MULTIPLE_MORBIDITIES').alias('MULTIPLE_MORBIDITIES'))\n",
    "\n",
    "st.bar_chart(age_bucket.sort('AGE_BUCKET'),x='AGE_BUCKET',y='MULTIPLE_MORBIDITIES')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80c174-f023-4c5f-89cf-374260293be1",
   "metadata": {
    "collapsed": false,
    "name": "title_h3",
    "resultHeight": 67
   },
   "source": [
    "The Third Bucket is a H3 bucket.  Grouping all the places to a H3 index.  This will create hexagons accross england.  The function we are using for this grouping is **H3_LATLNG_TO_CELL_STRING**. This will represent a hexagon shaped 'bucket' at a given resolution (size).  Visualising this sort of data is best performed using a map.  **Pydeck** is a useful python library for visualising location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25302e11-ec15-4f62-b583-ac1d4e4534f4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "h3_visual",
    "resultHeight": 423
   },
   "outputs": [],
   "source": [
    "import pydeck as pdk\n",
    "H3 = dataframe.select('MULTIPLE_MORBIDITIES','LAT','LON',\n",
    "                                 call_function('H3_LATLNG_TO_CELL_STRING',\n",
    "                                               col('LAT'),col('LON'),5).alias('H3'))\n",
    "H3 = H3.group_by('H3').agg(sum('MULTIPLE_MORBIDITIES').alias('MULTIPLE_MORBIDITIES'), avg('LAT').alias('LAT'),\n",
    "                          avg('LON').alias('LON'))\n",
    "\n",
    "TOTALS = H3.agg(avg('LAT').alias('LAT'),\n",
    "                         avg('LON').alias('LON'),\n",
    "                         max('MULTIPLE_MORBIDITIES').alias('MULTIPLE_MORBIDITIES')).to_pandas()\n",
    "\n",
    "tot_morbidities = float(TOTALS.MULTIPLE_MORBIDITIES.iloc[0])\n",
    "\n",
    "\n",
    "H3 = H3.with_column('M_PERC', div0(col('MULTIPLE_MORBIDITIES'),lit(tot_morbidities)))\n",
    "h3pd = H3.to_pandas()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "morbidities = pdk.Layer(\n",
    "        \"H3HexagonLayer\",\n",
    "        h3pd,\n",
    "        pickable=True,\n",
    "        stroked=True,\n",
    "        filled=True,\n",
    "        extruded=False,\n",
    "        get_hexagon=\"H3\",\n",
    "        get_fill_color=[f\"255 * M_PERC\",\"50\",\"140 - 140 * M_PERC\"],\n",
    "        line_width_min_pixels=2,\n",
    "        opacity=0.4)\n",
    "\n",
    "\n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=TOTALS.LAT.iloc[0],\n",
    "        longitude=TOTALS.LON.iloc[0],\n",
    "        zoom=5,\n",
    "        height=400\n",
    "        ),\n",
    "    \n",
    "layers= [morbidities], tooltip = {'text':\"Morbidities: {MULTIPLE_MORBIDITIES}\"}\n",
    "\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13a11f-288f-4bc5-92d6-117867fb38dd",
   "metadata": {
    "collapsed": false,
    "name": "heading_api_deprivation",
    "resultHeight": 114
   },
   "source": [
    "#### Get another Dataset from the gov website, create a dataframe and join to our existing dataset.\n",
    "Before we get anything from the internet we will need to enable an existing **External Access Integration**. \n",
    "\n",
    "- Click on the 3 dots to on the top right hand corner of the notebook and navigate to **notebook settings**\n",
    "- Under external access, enable **UK_GOV_PUBLISHING_SERVICE**\n",
    "- Also, enable **GEOPORTAL** external integration which is used in a later step.\n",
    "\n",
    "The next step downloads the CSV using the python CSV and requests packages and resultset is then loaded into a snowpark dataframe.  you will note that i am using **cache_result**.  this effectively inserts all the data in a temporary table which allows us to call it back quickly.\n",
    "\n",
    "Just to highlight, we are using a standard python **requests** package to call data externally. We then use the **csv** library to read the csv file, delimeter it as a comma, store it as a variable called my_list.  Finally, we will import that list into a new snowpark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d906478-cc9e-4efe-9582-0a53b0db615c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "get_data_from_csv",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# We can also use Snowpark for our analyses!\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "session = get_active_session()\n",
    "\n",
    "CSV_URL = 'https://assets.publishing.service.gov.uk/media/5dc407b440f0b6379a7acc8d/File_7_-_All_IoD2019_Scores__Ranks__Deciles_and_Population_Denominators_3.csv'\n",
    "\n",
    "\n",
    "with requests.Session() as s:\n",
    "    download = s.get(CSV_URL)\n",
    "\n",
    "    decoded_content = download.content.decode('utf-8')\n",
    "    cr = csv.DictReader(decoded_content.splitlines(), delimiter=',')\n",
    "    my_list = list(cr)\n",
    "\n",
    "\n",
    "deprivation = session.create_dataframe(my_list).cache_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce720c-5670-4456-9d96-a64ba24f7aab",
   "metadata": {
    "collapsed": false,
    "name": "view_results"
   },
   "source": [
    "Lets view the new dataframe as well as it's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4d28b-ad77-4177-8031-4651a141a2ac",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "view_the_data",
    "resultHeight": 1998
   },
   "outputs": [],
   "source": [
    "st.markdown('A preview of the data')\n",
    "st.write(deprivation.limit(5))\n",
    "\n",
    "st.markdown('All the columns')\n",
    "st.write(deprivation.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209c21d-0013-47bd-b3ae-0ba27a8d4712",
   "metadata": {
    "collapsed": false,
    "name": "selecting_columns_same_way"
   },
   "source": [
    "We have selected columns previously, the same can be applied to this newly created dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059c252-51cd-481d-86c2-1a5b882c3b74",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "view_deprivation",
    "resultHeight": 277
   },
   "outputs": [],
   "source": [
    "s_deprivation = deprivation.select(\"\\\"LSOA code (2011)\\\"\",\"\\\"LSOA name (2011)\\\"\",\n",
    "                                   '\"Health Deprivation and Disability Score\"',\n",
    "                                   '\"Living Environment Score\"',\n",
    "                                   '\"Total population: mid 2015 (excluding prisoners)\"')\n",
    "st.write(s_deprivation.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade0443-f402-42c5-8056-c33bef7a2272",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "group_orginal_dataset",
    "resultHeight": 464
   },
   "outputs": [],
   "source": [
    "st.write('Before we can sensibly join the datasets together, we must group the original dataset so its at the same level of aggregation as the deprivation dataset')\n",
    "dataframe = session.table('DEFAULT_DATABASE.DEFAULT_SCHEMA.\"Population Health Synthetic Data_over_5\"')\n",
    "data_grouped = dataframe.group_by('LSOA_CODE','ICB22NM').agg(sum('MULTIPLE_MORBIDITIES').alias('MULTIPLE_MORBIDITIES'),sum('ASTHMA').alias('ASTHMA'))\n",
    "data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f91e5-d16b-4570-b694-5bf44c97d2ef",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "join_data_frames",
    "resultHeight": 464
   },
   "outputs": [],
   "source": [
    "st.markdown('Next, we join the two dataframes together.')\n",
    "\n",
    "all_data = data_grouped.join(s_deprivation,data_grouped['LSOA_CODE']==s_deprivation[\"\\\"LSOA code (2011)\\\"\"]).drop(\"\\\"LSOA code (2011)\\\"\")\n",
    "\n",
    "all_data.write.mode('overwrite').save_as_table('DEFAULT_SCHEMA.\"Morbidities and Deprivation by LSOA\"')\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acdf8b-a7fa-4ad5-ad3f-1c07b1b07f6a",
   "metadata": {
    "collapsed": false,
    "name": "title_polygons",
    "resultHeight": 88
   },
   "source": [
    "#### Add Polygons to represent the LSOA boundaries and join to dataset\n",
    "\n",
    "Same as before, we need to get the polygons externally so we will need to create a network integration first.  Once this is in place, you simply allow the notebook to use it.  You need to ensure that the 'geoportal' integration is enabled for the notebook.\n",
    "\n",
    "This integration leveraging the geoservice to get the data we need from the ons geoportal\n",
    "https://geoportal.statistics.gov.uk/datasets/ons::lower-layer-super-output-areas-december-2011-boundaries-ew-bsc-v4/about\n",
    "\n",
    "Geopandas is used to format the data into binary format, then pushed it as a snowpark dataframe.  I then converted the binary polygon into a snowflake geography column and then only selected the LSOA code and the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9dd15-ff9c-4381-9770-b0de3dfc56ab",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "get_polygons",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# We can also use Snowpark for our analyses!\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "session = get_active_session()\n",
    "\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "# URL of the ArcGIS web service (replace with your actual URL)\n",
    "url='https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/LSOA_2011_Boundaries_Super_Generalised_Clipped_BSC_EW_V4/FeatureServer/0/query'\n",
    "\n",
    "params = {\n",
    "    \"where\": \"1=1\",  # Example query to fetch all records\n",
    "    \"outFields\": \"*\",  # Fetch all fields\n",
    "    \"f\": \"geojson\"  # Specify the format as GeoJSON\n",
    "}\n",
    "\n",
    "# Make the request to the ArcGIS web service\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Load the GeoJSON data into a GeoDataFrame\n",
    "    gdf = gpd.read_file(response.text)\n",
    "    \n",
    "    # Display the first few rows of the GeoDataFrame\n",
    "    polygons = session.create_dataframe(gdf.to_wkb()).cache_result()\n",
    "    polygons = polygons.with_column('GEOM',to_geography(col('\"geometry\"'))).select('LSOA11CD','GEOM')\n",
    "else:\n",
    "    print(\"Failed to fetch data:\", response.status_code)\n",
    "\n",
    "polygons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70d672-30df-4f53-9faf-5a0bd23850c7",
   "metadata": {
    "collapsed": false,
    "name": "cell1",
    "resultHeight": 114
   },
   "source": [
    "#### Creating a function in Python\n",
    "\n",
    "You can convert this python to a function which you can use in SQL too.  Converting to a function allows the logic to be reused.  in this case we are reusing the functionality to create a connector to the geoportal api.  This function is utilising python but you can create functions in java or sql too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a63d4-74d3-4f16-8f32-4b931af8ce5b",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "create_function",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION DEFAULT_SCHEMA.GET_LSOA_POLYGON()\n",
    "RETURNS TABLE (LSOA11CD STRING, GEOM string)\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "HANDLER = 'arcgisservicedata'\n",
    "PACKAGES = ('requests', 'geopandas', 'shapely')\n",
    "EXTERNAL_ACCESS_INTEGRATIONS = (GEOPORTAL)\n",
    "AS\n",
    "$$\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from shapely import wkb\n",
    "import pandas as pd\n",
    "\n",
    "class arcgisservicedata:\n",
    "    def process(self):\n",
    "        url = 'https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/LSOA_2011_Boundaries_Super_Generalised_Clipped_BSC_EW_V4/FeatureServer/0/query'\n",
    "                  \n",
    "    \n",
    "        # Parameters for the request\n",
    "        params = {\n",
    "        \"where\": \"1=1\",        # Example query to fetch all records\n",
    "        \"outFields\": \"*\",      # Fetch all fields\n",
    "        \"f\": \"geojson\"         # Specify the format as GeoJSON\n",
    "        }\n",
    "\n",
    "        # Fetch data from the ArcGIS service\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check for a successful response\n",
    "        if response.status_code == 200:\n",
    "            # Load the GeoJSON data into a GeoDataFrame\n",
    "            gdf = gpd.read_file(response.text)\n",
    "\n",
    "        # Convert the geometry to WKB format and prepare output data\n",
    "            gdf['GEOM'] = gdf['geometry'].apply(lambda x: x.wkb_hex)\n",
    "        \n",
    "        # Prepare columns to output\n",
    "            output = pd.DataFrame({\n",
    "                'LSOA11CD': gdf['LSOA11CD'],\n",
    "                'GEOM': gdf['GEOM']\n",
    "            })\n",
    "        \n",
    "            # Yield each row as a tuple\n",
    "            for _, row in output.iterrows():\n",
    "                yield (row['LSOA11CD'], row['GEOM'])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to fetch data: {response.status_code}\")\n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b9b28-54b3-45e8-854a-8f91755a3a59",
   "metadata": {
    "collapsed": false,
    "name": "cell3",
    "resultHeight": 47
   },
   "source": [
    "#### Using the Created function in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1b7b3-170d-42b3-abdd-bf12fa20b2fc",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "using_function_sql",
    "resultHeight": 510
   },
   "outputs": [],
   "source": [
    "select LSOA11CD, to_geography(GEOM) from table(DEFAULT_SCHEMA.GET_LSOA_POLYGON())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9020a-d9e8-41bc-9520-957a69454663",
   "metadata": {
    "collapsed": false,
    "name": "join_to_data",
    "resultHeight": 67
   },
   "source": [
    "We are now using the join feature to join the polygons back onto the dataset.  This dataframe is now ready for converting a persisted table which can then be used in other visualisation tools such as tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdb2cb-38f8-4ab3-9825-377d2dd08aef",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "join_polygons_to_data",
    "resultHeight": 426
   },
   "outputs": [],
   "source": [
    "all_data = session.table('DEFAULT_SCHEMA.\"Morbidities and Deprivation by LSOA\"')\n",
    "\n",
    "all_data_polygons = all_data.join(polygons,all_data['LSOA_CODE']==polygons['LSOA11CD']).drop('LSOA11CD')\n",
    "\n",
    "all_data_polygons.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bb4de-08c8-477e-aa92-28a18ca44ff7",
   "metadata": {
    "collapsed": false,
    "name": "create_table",
    "resultHeight": 41
   },
   "source": [
    "Create a new table from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278f947-fccb-457f-9f0e-b7a5317a83c6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "save_table",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "all_data_polygons.write.mode('overwrite').save_as_table('\"Health and Deprivation by LSOA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ed959-4d46-422b-879a-5250ba35068b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "call_table",
    "resultHeight": 277
   },
   "outputs": [],
   "source": [
    "st.markdown('Here, we can now call the table into a new dataframe')\n",
    "\n",
    "session.table('\"Health and Deprivation by LSOA\"').limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82e501-ac2d-4133-8704-dd000a326c22",
   "metadata": {
    "collapsed": false,
    "name": "cell2",
    "resultHeight": 67
   },
   "source": [
    "And now we have seaved as a table, we can view the results in SQL.  You will also note that I leveraged the AS_ASWKT function which is useful for rendering in **matplotlib** and also Power BI's icon map.  https://www.iconmappro.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce234a3d-1544-4a56-bfef-63dd7c55f693",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "geo",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "select *,ST_ASWKT(GEOM)WKT from \"Health and Deprivation by LSOA\"  WHERE ICB22NM like '%London Integrated Care Board';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed13851-4e0e-48f4-9917-0de11cf2aa14",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "london_comparisons",
    "resultHeight": 492
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "\n",
    "st.markdown('Here is a comparison of london, comparing with environmental scores with **Asthma** and those with \\\n",
    "**Multiple Morbidities**.  **NB** There are gaps in the underlying data which is useful way of easily spotting quality issues. I believe the gaps might be to do with codeing changes')\n",
    "# Convert geo DataFrame to GeoPandas DataFrame\n",
    "geom = gpd.GeoDataFrame(geo.to_df().to_pandas())\n",
    "\n",
    "# Set geometry from WKT and define CRS\n",
    "geodframe = geom.set_geometry(gpd.GeoSeries.from_wkt(geom['WKT']))\n",
    "geodframe.crs = \"EPSG:4326\"\n",
    "\n",
    "# Create three columns in Streamlit\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "# Plot Living Environment Score\n",
    "with col1:\n",
    "    fig1, ax1 = plt.subplots(1)\n",
    "    ax1.axis('off')\n",
    "    geodframe.plot(column='Living Environment Score', cmap='Reds', alpha=1, ax=ax1)\n",
    "    ax1.set_title('Living Environment Score')\n",
    "    st.pyplot(fig1)\n",
    "    plt.close(fig1)\n",
    "\n",
    "# Plot ASTHMA with Greens colormap\n",
    "with col2:\n",
    "    fig2, ax2 = plt.subplots(1)\n",
    "    ax2.axis('off')\n",
    "    geodframe.plot(column='ASTHMA', cmap='Greens', alpha=1, ax=ax2)\n",
    "    ax2.set_title('Asthma Sufferers')\n",
    "    st.pyplot(fig2)\n",
    "    plt.close(fig2)\n",
    "\n",
    "# Plot ASTHMA with Oranges colormap\n",
    "with col3:\n",
    "    fig3, ax3 = plt.subplots(1)\n",
    "    ax3.axis('off')\n",
    "    geodframe.plot(column='MULTIPLE_MORBIDITIES', cmap='Oranges', alpha=1, ax=ax3)\n",
    "    ax3.set_title('Multiple Morbidities')\n",
    "    st.pyplot(fig3)\n",
    "    plt.close(fig3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d934c7-d2a5-4145-8a8f-b70a26ae2a34",
   "metadata": {
    "collapsed": false,
    "name": "Streamlit"
   },
   "source": [
    "As you have been going through this notebook, you will have seen how some of the streamlit objects work.  The last cell you loaded introduced you to visualising maps, as well as putting each map in one of 3 columns, and applying markdown as narritive.  All of this work can be used as a starting point to build a streamlit application.  Please now navigate to **Projects > Streamlits > Population_Health** and see what can be achieved.  Streamlit is a python based application framework which is designed for analysts who know python and want to share their data application to the wider community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "879167745201",
   "authorName": "USER",
   "lastEditTime": 1744033226928,
   "notebookId": "xsagwobygvhsxop2dc6m",
   "sessionId": "868351b4-1592-4a79-886a-525dc94a2d6c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
